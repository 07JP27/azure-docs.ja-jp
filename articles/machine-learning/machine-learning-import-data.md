<properties
	pageTitle="Azure Machine Learning Studio にトレーニング データをインポートする | Azure"
	description="さまざまなデータ ソースから Azure Machine Learning Studio にトレーニング データをインポートする方法"
	services="machine-learning"
	documentationCenter=""
	authors="garyericson"
	manager="paulettm"
	editor="cgronlun"/>

<tags
	ms.service="machine-learning"
	ms.workload="data-services"
	ms.tgt_pltfrm="na"
	ms.devlang="na"
	ms.topic="article"
	ms.date="04/21/2015"
	ms.author="garye" />


#Azure Machine Learning Studio にトレーニング データをインポートする

Azure Machine Learning Studio で予測分析ソリューションを開発する場合、問題のある領域の代表的なデータを使用してモデルをトレーニングします。ML Studio には、この目的で使用できるデータセットのサンプルが多数用意されています (「[Azure Machine Learning Studio におけるサンプル データセットの使用](machine-learning-use-sample-datasets.md)」をご覧ください)。また、独自のデータを ML Studio にインポートして実験に使用することもできます。

[AZURE.INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]

独自のデータを ML Studio で使用するために、事前にローカル ハード ドライブからデータ ファイルをアップロードして、データセット モジュールをワークスペースで作成できます。あるいは、[リーダー][reader] モジュールで実験を実行している間に、いずれかのオンライン ソースからデータにアクセスすることもできます。

- Azure BLOB ストレージ、テーブル、SQL データベース
- HiveQL を使用した Hadoop
- HTTP を使用した Web URL
- データ フィード プロバイダー

ML Studio は、区切られたテキスト データやデータベースからの構造化されたデータなど、四角形のデータや表形式のデータで作業するように設計されています。場合によっては、四角形以外のデータが使用されることもあります。

比較的クリーンなデータが最適です。具体的には、実験にデータをアップロードする前に、引用符で囲まれていない文字列などを処理しておくことをお勧めします。ただし、ML Studio には、実験内でデータを操作できるモジュールが用意されています。使用する機械学習アルゴリズムに応じて、値の欠落やスパース データなどのデータ構造上の問題を処理する際に、その対処方法を決定する必要があります。そのような場合に、これらのモジュールが役に立ちます。これらの関数を実行するモジュールは、モジュール パレットの **[Data Transformation]** セクションで確認します。

実験中は、出力ポートを右クリックして、モジュールで生成されたデータをいつでも表示、ダウンロードできます。モジュールによっては、使用可能なダウンロードのオプションが異なる場合があります。また、ML Studio 上でデータを Web ブラウザーに表示できる場合もあります。


## データ形式

データのインポートに使用するメカニズムや、データの送信元に応じて、さまざまなデータ型を実験にインポートできます。

- プレーン テキスト (.txt)
- コンマ区切り値 (CSV) - ヘッダー付き (.csv) またはヘッダーなし (.nh.csv)
- タブ区切り値 (TSV) - ヘッダー付き (.tsv) またはヘッダーなし (.nh.tsv)
- Hive テーブル
- SQL データベース テーブル
- OData 値
- SVMLight データ (.svmlight) (形式の詳細については、「[SVMLight の定義](http://svmlight.joachims.org/)」をご覧ください)。
- 属性関係ファイル形式 (ARFF) データ (.arff) (「[ARFF の定義](http://weka.wikispaces.com/ARFF)」をご覧ください)。
- Zip ファイル (.zip)
- R オブジェクトまたはワークスペース ファイル (.RData)

メタデータを含む ARFF などの形式でデータをインポートする場合、ML Studio はこのメタデータを使用して各列の見出しとデータ型を定義します。このメタデータが含まれていない TSV や CSV 形式などのデータをインポートする場合、ML Studio は、データをサンプリングすることによって、各列のデータ型を推論します。また、データに列見出しがない場合、ML Studio は既定の名前を提供します。[メタデータ エディター][metadata-editor]を使用して、列の見出しやデータ型を明示的に指定、変更できます。
 
次のデータ型は、ML Studio によって認識されます。

- String
- 整数
- Double
- Boolean
- DateTime
- TimeSpan

ML Studioは*データ テーブル*と呼ばれる内部データ型を使用し、モジュール間でデータを渡します。[データセットへの変換][convert-to-dataset]モジュールを使用して、データ テーブル形式にデータを明示的に変換できます。データ テーブル以外の形式を受け取るどのモジュールでも、次のモジュールに渡す前に、サイレント モードでデータをデータ テーブルに変換します。必要に応じて、その他の変換モジュールを使用して CSV、TSV、ARFF、SVMLight 形式にデータ テーブル形式をもう一度変換できます。これらの関数を実行するモジュールについては、モジュール パレットの **[Data Format Conversions]** セクションを確認してください。


## ローカル データ ファイルのインポート

次の手順に従って、ローカルのハード ドライブからデータをアップロードできます。

1. ML Studio ウィンドウの下部で、**[+新規]** をクリックします。
2. **[データセット]** と **[FROM LOCAL FILE]** を選択します。
3. **[Upload a new dataset]** ダイアログ ボックスで、アップロードするファイルを参照します。
4. 名前を入力し、データ型を指定したら、必要に応じて説明を入力します。データの特徴を記録しておくと、後でデータを使用する際に参照できるため、説明を入力しておくことをお勧めします。
5. チェックボックス **[This is the new version of an existing dataset]** をオンにしておくことにより、新しいデータで既存のデータセットを更新できます。このチェックボックスをクリックし、既存のデータセット名を入力するだけです。

アップロードする際に、ファイルがアップロードされていることを示すメッセージが表示されます。アップロード時間は、データのサイズと、サービスへの接続速度に依存します。ファイルのアップロードに時間がかかる場合、待機している間に ML Studio で他の作業も実行できます。ただし、ブラウザーを閉じるとデータのアップロードに失敗します。

データがアップロードされると、データセット モジュールに格納され、ワークスペース内のすべての実験で使用できるようになります。実験の編集中に、モジュール パレットの **[保存されたデータセット]** リストに、あらかじめ読み込まれているすべてのサンプル データセットやデータセットが表示されます。


## リーダー モジュールを使用してオンライン データにアクセスします。

実験に[リーダー][reader] モジュールを使用すると、実験の実行中にいくつかのオンライン ソースからデータにアクセスできます。このデータへは実験の実行中にアクセスするため、1 つの実験でのみ使用できます (ワークスペース内のすべての実験で使用できるデータセット モジュールとは異なります)。

[リーダー][reader] モジュールを実験に追加した後、**データ ソース**を選択してから、モジュール パラメーターを使用してアクセス情報を指定します。たとえば、**HTTP 経由で Web URL **を選択した場合は、ソース URL とデータ形式を指定します。Azure ストレージや HDInsight (Hive クエリを使用) からデータにアクセスしている場合は、該当するアカウント情報とデータの場所を指定します。

> [AZURE.NOTE]この記事では、[リーダー][reader] モジュールに関する一般的な情報を記載しています。アクセス可能なデータの種類、形式、パラメーターの詳細や、よく寄せられる質問への回答については、「[リーダー][reader]」モジュールの概要をご覧ください。


### Azure からのデータの取得

次の 3 つの Azure ソースからデータをインポートできます。

- **Azure BLOB ストレージ** - ストレージに ARFF 形式を使用する場合、列は、ヘッダーのメタデータを使用してマップされます。TSV 形式または CSV 形式を使用する場合、列データをサンプリングすることによってマッピングが推測されます。 
- **Azure テーブル ストレージ** - [リーダー][reader] モジュールでデータをスキャンし、列のデータ型を識別します。データがほとんど同種で予測可能な場合は、スキャンされる行の数を制限できます。
- **Azure SQL データベース** - [リーダー][reader] モジュールは、指定したデータベース クエリを使用して、データのインポートに SQL Azure Transact クライアント API を活用します。

BLOB とテーブル ストレージの場合は、データへのアクセスを提供するために、Shared Access Signature URI (SAS URI) または Azure ストレージ アカウント情報を指定します。Azure SQL データベースの場合は、データベースとアカウント情報に加え、インポートするデータを識別するデータベース クエリを指定します。

### Web からのデータの取得

[リーダー][reader] モジュールを使用して、Web や FTP サイトからデータを読み取ります。次のものを指定する必要があります。

- ファイルの完全な HTTP URL アドレス
- ファイルのデータ形式 (CSV、TSV、ARFF、SvmLight)
- CSV または TSV ファイルの場合、ファイルの最初の行がヘッダーであるかどうかを示します。

### Hadoop からのデータの取得

[リーダー][reader] モジュールでは、HiveQL クエリ言語を使用して分散ストレージからデータを読み取ることができます。Hive データベースのクエリを指定し、HCatalog サーバー上でユーザーのアクセス情報を入力する必要があります。また、データを Hadoop 分散ファイル システム (HDFS) と Azure のどちらに保存するかを指定し、Azure に保存する場合は、Azure のアカウント情報を指定する必要があります。

### データ フィード プロバイダーからのデータの取得

OData URL を指定することによって、データ フィード プロバイダーから直接読み取ることできます。ソース URL とデータ コンテンツの種類を指定する必要があります。


## 実験からのデータの保存


実験の中間結果を別の実験の一部として使用することが必要になる場合があります。これを行うには、次の手順を実行します。

1. データセットとして保存するモジュールの出力を右クリックします。

2. **[データセットとして保存]** をクリックします。

3. メッセージが表示されたら、名前と、データセットを簡単に識別できるような説明を入力します。

4. **[OK]** チェックマークをクリックします。

保存が完了すると、ワークスペースのどの実験でもデータセットを使用できるようになります。データセットは、モジュール パレットの **[保存されたデータセット]** リストから検索できます。


<!-- Module References -->
[convert-to-dataset]: https://msdn.microsoft.com/library/azure/72bf58e0-fc87-4bb1-9704-f1805003b975/
[metadata-editor]: https://msdn.microsoft.com/library/azure/370b6676-c11c-486f-bf73-35349f842a66/
[reader]: https://msdn.microsoft.com/library/azure/4e1b0fe6-aded-4b3f-a36f-39b8862b9004/

<!--HONumber=54--> 