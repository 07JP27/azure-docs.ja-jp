<properties title="Azure Machine Learning API service operations" pageTitle="Machine Learning API service operations | Azure" description="Creating and managing Azure Machine Learning web services" metaKeywords="" services="" solutions="" documentationCenter="" authors="derrickv" videoId="" scriptId="" />

<tags ms.service="machine-learning" ms.workload="tbd" ms.tgt_pltfrm="na" ms.devlang="na" ms.topic="article" ms.date="01/01/1900" ms.author="derrickv"></tags>

# Azure Machine Learning API サービスの操作

一般的な Microsoft Azure Machine Learning (Azure ML) プロジェクトは、大まかには次の手順を必要とします。

1.  データの取得、分析、準備
2.  さまざまな ML アルゴリズムを活用した Machine Learning の実験の作成
3.  トレーニング、テスト、トレーニング済みのモデルの生成
4.  トレーニング済みのモデルを使用した運用ワークフローの作成と運用環境へのデプロイ
5.  モデルのパフォーマンスと以降の更新の監視

> 用語 "実験" は、無閉路有効グラフ (DAG) の形式の、データの入力と操作、トレーナー、スコアラーが関係する対話型のワークフローを説明するために使用されます。ワークフローが Azure Web サービスとして発行されると対話型ではなくなり、変更するには、モデルを更新した後に、再発行して Web サービスとその動作を更新する必要があります。

手順 1. から 3. は通常、データ サイエンティストによって何度も繰り返し実行されます。それが終わると ML モデルはエンジニアリング チームおよび運用チームに渡されて運用環境のシステムに統合され、運用環境で使用できるようになります。

ML モデルを運用環境のシステムに統合およびデプロイする従来のプロセスは、R、Python、C#、Java などのモデルを作成するコード、プラットフォームの統合やインフラストラクチャに関する考慮事項、およびデプロイメントの計画によっては、数週間または数か月かかることもあります。

Azure ML は、モデルの作成と評価を簡単かつ直観的な作業へと変え、実験を Azure の Web サービスとしてデプロイするための単純なプロセスを提供し、その結果、モデルの実験から運用環境で Web サービスとして実行するまでの合計時間を大幅に削減することで、このプロセスを簡略化し合理化しています。

このドキュメントでは、ML の実験から Azure ML Web サービスを設定する際の概念と手順を説明します。

# Azure ML プロセスの概要

Azure ML により、Azure Machine Learning Studio (ML Studio) に定義された ML の実験から Web サービスを作成できます。Azure ML Web サービスは、リアルタイムまたはバッチ モードの実際の入力データに基づいた予測に使用できます。

次の図では、大まかな手順を 2 つに分けて示します。1 つ目がモデルの作成で、2 つ目が Web サービスとしてのそのモデルの発行です。このドキュメントでは、図 1 の右側、つまり、スコアを付ける Web サービスの発行を重点的に取り扱い、そのプロセスに関係する概念について説明します。

![][]

図 1: スコアを付ける Web サービスのプロビジョニング、作成、発行

# Azure ML Web サービス

Machine Learning (ML) のコンテキストでは、Web サービスは、外部アプリケーションと Machine Learning のワークフローの間の通信を提供するソフトウェア インターフェイスです。予測の結果を受信し、その結果を外部クライアント アプリケーションに組み込むために、スコアを付けるモデルとリアルタイムで通信する手段を提供します。Azure ML では、Azure ML Web サービスのデプロイメント、ホスティング、および管理に Microsoft Azure が活用されます。Azure ML のサービスを使用して、2 種類のサービスを作成できます。

## 要求応答サービス (RRS)

要求応答サービス (RRS) は、待ち時間が短く、拡張性の高い Web サービスであり、実験環境で作成および発行されたステートレスなモデルへのインターフェイスを提供するために使用されます。

-   REST API: RRS は REST ベースの Web サービスです。
-   サービス インターフェイス: RRS Web サービスのインターフェイスは、Azure ML Studio の実験の入出力ポートを使用して、実験の設定の一部として定義されています。
-   開発段階: Azure ML のワークフローの一部として、RRS サービスはまずステージング環境で生成され、ステージング環境でテストすることができます。完成し、運用の準備ができたと見なされると、運用環境にデプロイされます。
-   Azure でデプロイ: RRS をデプロイすると、Azure Web サービスのエンドポイントになります。
-   インターフェイス パラメーター: RRS サービスへの要求は、Studio の定義された実験を使用してスコア付けされる必要があるデータです。応答は、モデルの予測の結果です。
-   応答の値: RRS は、単一行の入力パラメーターを受け取り、単一行を出力パラメーターとして生成します。出力行には複数列が含まれる可能性があります。

## バッチ実行サービス (BES)

バッチ実行サービス (BES) は、データ レコードのバッチに対して非同期でスコアを付けるためのサービスです。BES への入力は、RRS で使用されるデータ入力と似ています。主な違いは、BES が、BLOB、Azure のテーブル、SQL Azure、HD Insight (Hive Query)、HTTP ソースなどのさまざまなソースからレコードのバッチを読み取る点です。スコア付けの結果は Azure BLOB ストレージのファイルに出力され、ストレージのエンドポイントのデータが応答で返されます。

BES では、実行中のスコア付けのプロセスのステータスを取得したり、要求を取り消したりするためのインターフェイスも提供されます。BES では、きわめて大量のデータに対してモデルのパッケージを実行できます。

-   REST API: BES は REST ベースの Web サービスです。
-   サービス インターフェイス: RRS と同様に、BES Web サービスのインターフェイスは、Azure ML Studio の実験の入出力ポートを使用して、モデルの設定の一部として定義されています。
-   開発段階: BES を作成するワークフローの一部として、まずステージング環境で生成され、ステージング環境でテストすることができます。完成し、運用の準備ができると、運用環境にデプロイされます。
-   Azure でデプロイ: BES をデプロイすると、Azure Web サービスのエンドポイントになります。
-   インターフェイス パラメーター: BES サービスへの要求は、Azure BLOB のファイルまたはスコア付けされるレコードの SAS 入力の URL です。応答は Azure BLOB に書き込まれ、応答のストレージ エンドポイントの URL が返されます。

# Azure ML Web サービスの発行

Azure ML Studio ではブラウザー ベースのアプリケーションが提供され、さまざまなデータ ソース、データ操作および検証モジュール、ML アルゴリズムを使用して、グラフィック ユーザー インターフェイスから機械学習の実験を容易に作成し実行できます。ML Studio での実験は、データ処理モジュールの無閉路有効グラフ (DAG) として構築します。

実験を設定し、データで正常にトレーニングを実行すると、トレーニング済みのモデルとして保存してスコア付けに使用することができます。トレーニング済みのモデルはその後、スコア付け実験やワークフローで使用して、Azure Web サービスとして発行します。

## トレーニング実験

実験では、データの読み込みと操作、Machine Learning のアルゴリズムの適用、結果の評価のためのさまざまなモジュールを使用する場合があります。"モデルのトレーニング" では、トレーニング データセットと学習アルゴリズムが使用して応答を予測します。

モデルの実行が正常に終了すると、テスト データセットとクエリをスコア付けするための再利用可能なコンポーネントとして "モデルのトレーニング" を保存することができます。

![][1]

図 2: 実験でのトレーニング済みのモデルの保存を示すサンプル

保存したトレーニング済みのモデルはその後、アプリケーションの [トレーニング済みのモデル] セクションで利用できるようになります。

![][2]

図 3: モデルの一覧を示す [トレーニング済みのモデル] セクション

## スコア付け実験

スコア付け実験により、トレーニング済みのモデルとサンプル データを使用して予測が生成されます。

上の図 1 は、実験での "モデルのスコア付け" の使用法を示します。Studio では、これは Machine Learning のモジュールの一部です。

![][3]

図 4: モデルのスコア付け

### 要求応答サービスとバッチ実行サービス

Web サービスとして発行される、スコア付け実験を作成する際に、スコア付けのシナリオに応じて 2 つのうちいずれかを選択できます。スコアを付ける要求が、顧客 A が業者を切り替えるかどうかを判断する要求 (顧客離れの予測) など、単一レコードのスコア付けを必要とする場合、このレコードはリアルタイムでスコア付けでき、RRS Web サービスとして作成されることになります。サービスはリアルタイムで予測モデルの結果を返します。顧客離れの予測の例であれば、この回答は、"はい" か "いいえ" になります。

顧客データが含まれるレコードのバッチがサービスに送信されてスコア付けされる場合のような、1 つの要求で多くのレコードをスコア付けする必要があるスコア付けの操作では、適切なサービスは BES です。この場合の要求は、非同期な要求になります。この要求では、すべてのレコードが処理されて、すべての処理が完了した後に応答が返される前に Azure BLOB に保存されます。

### トレーニング済みのモデルの使用

スコア付け実験を設定するために、トレーニング済みのモデル (図 3 の "成人収入予測") が実験に追加されます。トレーニング済みのモデルのトレーニングに使用される他のモジュールは削除できます。最終的なワークフローは、下の図 5 のようになります。

### 入出力ポート

トレーニング済みのモデル (詳細は上のセクション参照) を使用した実験の設定と更新済みの実験のスコア付けの後に、入出力ポートを設定して、予測モデルへのデータと予測の結果への開始ポイントと終了ポイントを指定する必要があります。これは、発行済みの Web サービスのインターフェイスの定義として動作します。"モデルのスコア付け" の入力ポートは、次のように開始ポイントを右クリックして設定します。

![][4]

図 5: 入力ポートのスコア付けの設定

同様に、"モデルのスコア付け" の出力ポートも次のように設定します。

![][5]

図 6: 出力ポートの設定

## サービスの発行

ポートを設定して実験を実行した後、モデルを Web サービスとして発行できます。最初に、サービスをステージング環境に発行し、運用環境のデプロイの準備をする前にテストして、予測した結果をサービスが返すことを確認します。

### ステージング環境への発行

[Web サービスの発行] アイコンをクリックすると、Web サービスがステージング環境にデプロイされます。

![][6]

図 7: [Web サービスの発行] ボタン

モデルを Web サービスとしてステージング環境に発行した後、入力パラメーターを使用してテストし、運用環境にデプロイするためのマークを付けることができます。ダッシュボードにはテストのリンクが示されます。

![][7]

図 8: Web サービス ダッシュボード

[テスト] リンクをクリックして、スコア付けのパラメーターを入力すると、Web サービスをステージング環境でテストできます。テストの要求は、モデルを使用し、入力したデータに基づいてスコア付けされ、スコア付けの結果が返されます。

### 運用環境への発行

Web サービスを運用環境に発行すると、予測とスコア付けで使用するために他のアプリケーションで利用できるようになります。ステージング環境へのデプロイが終了し、テストが正常に実行された後に、運用環境へデプロイするために Web サービスにマークが付けられます。

![][8]

図 9: Web サービスへの運用デプロイの準備完了のマーク付け

これは実際にデプロイするのではなく、サービスを運用環境へデプロイするために、適切なデプロイのアクセス許可を持つユーザーへの通知を作成します。

![][9]

図 10: 運用環境へデプロイするためのデプロイの通知とオプション

## Web サービスの呼び出し

#### RRS

RRS Web サービスは REST のエンドポイントであり、さまざまなプログラミング言語を使用してクライアント アプリケーションから呼び出すことができます。API のヘルプ ページでは、新しい Web サービスを呼び出すためのサンプル コードへのリンクが提供され、C#、R、および Python のサンプルを示しています。

![][10]

図 11: RRS を呼び出すためのサンプル コード

## スコアを付けない実験

スコアを付ける Web サービスの作成に加えて、データ抽出や変換などの他のタスクを実行するための実験も作成できます。この場合、Web サービスでは機械学習の操作は実行されません。この Web サービスでは、Azure ML Studio のデータ操作機能を使用して、さまざまなデータ ソースの読み込み、データ型の変換、データ操作や数学的操作のフィルターと適用を実行します。

### スコアを付けない Web サービスの発行

スコアを付けない Web サービスの発行の手順は、前に説明したスコアを付けるサービスの発行の手順に類似しています。主な違いは、出力ポートが "モデルのスコア付け" で定義されていないことです。

# 発行済みのサービスの更新

トレーニング データの更新、トレーニングとスコア付けに使用されるデータ スキーマの変更、アルゴリズムの改善の必要性、元の ML モデルのその他の変更などのさまざまな理由から、発行済みの Web サービスを更新することが必要になる場合があります。これらの変更は、トレーニング済みのモデルとスコア付けの結果に影響を与え、更新済みの Web サービスの発行が必要になります。

![][11]

図 12: モデルの編集と更新済みのスコアを付ける Web サービスの発行

## トレーニング済みのモデルの更新

トレーニング実験を変更すると、トレーニング済みのモデルの再トレーニングが必要になります。そのためには、発行済みのモデルを編集する必要があります。下の例では、上の図 5 に示されたスコアを付けるワークフローから、既存のトレーニング済みのモデルが削除されています。

![][12]

図 13: ワークフローから削除されたトレーニング済みのモデル

次に、トレーニングとテストのセグメントへのデータの分割、学習アルゴリズムの適用、モデルのトレーニング、トレーニング データのスコア付け、および結果の評価に必要なモジュールが追加されます。これらは、別の学習アルゴリズムの適用の必要性などの、実験に必要とされるその他の変更に応じて変化する可能性があります (図 14)。

新しいモジュールが追加されると、実験を再実行するには、必要に応じて構成する必要があります。たとえば、下の図の赤い円は、ラベル列が "モデルのトレーニング" に設定されていないことを示します。

![][13]

図 14: モデルの再トレーニングのために追加されたモジュール

[モデルのトレーニング] をクリックし、ラベルの列名を設定すると、問題が解決されます。

![][14]

図 15: ラベルとしての収入列の選択

## 更新されたトレーニング済みのモデルの保存

すべての新しいモジュールを適切に構成した後に、実験を保存し正常に再実行する必要があります。"モデルのトレーニング" はその後で保存できます (上の図 2 を参照)。違いは、この場合は、既存のトレーニング済みのモデルを更新するためにチェック ボックスがオンになっている必要がある点です。

![][15]

図 16: 既存のトレーニング済みのモデルを更新する

## 更新済みのサービスの発行

トレーニング済みのモデルが更新されると、上のセクションの **Azure ML Web サービス**の発行で説明した手順が繰り返されます。

-   スコア付け実験に (先ほど更新した) トレーニング済みのモデルを使用する
-   入出力ポートを設定する
-   ステージング環境に発行する
-   運用環境に発行する

実験が更新され、新しいトレーニング済みのモデルが作成およびスコア付けされたら、[Web サービスの発行] をクリックするとサービスが発行されます。新しく作成されたサービスによって既存のサービスが上書きされます。



  []: ./media/machine-learning-overview-of-azure-ml-process/oamlp1.png
  [1]: ./media/machine-learning-overview-of-azure-ml-process/oamlp2.png
  [2]: ./media/machine-learning-overview-of-azure-ml-process/oamlp3.png
  [3]: ./media/machine-learning-overview-of-azure-ml-process/oamlp4.png
  [4]: ./media/machine-learning-overview-of-azure-ml-process/oamlp5.png
  [5]: ./media/machine-learning-overview-of-azure-ml-process/oamlp6.png
  [6]: ./media/machine-learning-overview-of-azure-ml-process/oamlp7.png
  [7]: ./media/machine-learning-overview-of-azure-ml-process/oamlp8.png
  [8]: ./media/machine-learning-overview-of-azure-ml-process/oamlp9.png
  [9]: ./media/machine-learning-overview-of-azure-ml-process/oamlp10.png
  [10]: ./media/machine-learning-overview-of-azure-ml-process/oamlp11.png
  [11]: ./media/machine-learning-overview-of-azure-ml-process/oamlp12.png
  [12]: ./media/machine-learning-overview-of-azure-ml-process/oamlp13.png
  [13]: ./media/machine-learning-overview-of-azure-ml-process/oamlp14.png
  [14]: ./media/machine-learning-overview-of-azure-ml-process/oamlp15.png
  [15]: ./media/machine-learning-overview-of-azure-ml-process/oamlp16.png
