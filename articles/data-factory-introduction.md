<properties title="Introduction to Azure Data Factory" pageTitle="Azure Data Factory の概要" description="Azure データ ファクトリ サービスを使用してデータ処理、データ保存、データ移動のサービスを構成し、信頼済みの情報を生成するパイプラインを作成する方法について説明します。" metaKeywords=""  services="data-factory" solutions=""  documentationCenter="" authors="spelluru" manager="jhubbard" editor="monicar" />

<tags ms.service="data-factory" ms.workload="data-services" ms.tgt_pltfrm="na" ms.devlang="na" ms.topic="article" ms.date="01/01/1900" ms.author="spelluru" />

# Azure Data Factory サービスの概要
**Azure Data Factory** サービスは、データの保存、処理、移動の各サービスを、効率的かつスケーラブルで信頼性の高いデータ生成パイプラインとして構成する、完全に管理されたサービスです。  開発者は Data Factory を使用して、内部設置型やクラウドのソースから取得した半構造化データ、非構造化データ、および構造化データを信頼できる情報に変換できます。また、内部設置型、クラウドベース、およびインターネット サービスから取得したデータを結合、集計、変換するデータ ドリブン ワークフロー (パイプライン) を構築し、シンプルな JSON スクリプトを使用して複雑なデータ処理を設定できます。Azure Data Factory サービスでは、Azure プレビュー ポータルが提供する充実したビジュアル エクスペリエンスを通じて、これらのパイプラインの監視と管理をひとめで行えます。パイプラインによって生成された情報は、BI ツールや分析ツールなどのアプリケーションで簡単に利用でき、重要なビジネスに関する洞察と意思決定を信頼できる方法で後押しします。

この記事では、Azure Data Factory サービス、もたらされる価値、およびサポートしているシナリオの概要について説明します。 

##Azure Data Factory の概要
これまでのデータ統合プロジェクトは、抽出/変換/読み込み (ETL) 処理を中心として展開されてきました。ETL とは、次の図に示すように、組織内の各種データ ソースからデータを抽出し、エンタープライズ データ ウェアハウス (EDW) のターゲット スキーマに準拠するようにデータを変換して、EDW にデータを読み込む処理です。この処理の後、EDW は、BI ソリューションや分析ソリューションから唯一の情報源としてアクセスされます。 

![Traditional ETL][image-data-factory-introduction-traditional-ETL]

今日の企業を取り巻くデータ環境は、次の図に示すように、量、種類、複雑さのいずれにおいても指数関数的に増大し続けています。形式もスピードも異なる内部設置型データとクラウドで生成されたデータが混ざり合い、かつてないほど多様化しています。データ処理は、各地に分散する場所にまたがって行わなければならず、オープン ソース ソフトウェア、商用ソリューション、カスタム処理サービスが併用され、高コストで、統合や保守が困難なものとなっています。変化し続ける現代のビッグ データ環境への適用に欠かせない機敏性は、従来の EDW に現代の情報生成システムとして求められる各種機能を組み合わせることで獲得できます。Azure Data Factory サービスは、従来の EDW と変化するデータ環境の両方に対して機能する構成プラットフォームであり、企業は利用可能なすべてのデータを活用してデータ主導の意思決定を行えるようになります。

![Todays Diverse Processing Landscape][image-data-factory-introduction-todays-diverse-processing-landspace]


企業は **Azure Data Factory** サービスを通じて、**データの処理、保存、および移動のサービスを情報生成パイプラインとして構成して、信頼できるデータ資産を管理する**プラットフォームを獲得でき、今日のデータ環境がもたらす多様性を活用できます。

Azure Data Factory サービスでは、以下を実行できます。

- **さまざまなデータ保存システムや処理システムと簡単に連携。** 
Data Factory では、データが保存されている場所でデータを使用し、保存、処理、およびデータ移動のサービスを構成できます。たとえば、情報生成パイプラインを構成し、SQL Server などの内部設置型データを Azure SQL データベース、BLOB、テーブルなどのクラウド データと併せて処理できます。  
- **データを信頼できる情報に変換。** 
複雑なデータの結合と整形は思いどおりの結果になるまで試行錯誤が必要だったり、データ モデルの変更にコストと時間がかかったりすることがあります。Data Factory を使用すると、サービスが組み込みを処理している間に変革に関する分析に集中できます。Data Factory は、Hive、Pig、および C# の処理に加えて、Hadoop (HDInsight) クラスターの自動管理、一時的な障害発生時の再試行、構成可能なタイムアウト ポリシー、アラート機能などの主要な処理機能をサポートしています。  
- **1 か所でデータ パイプラインを監視。** 
多様なデータ ポートフォリオを使用する場合は、保存、処理、およびデータ移動のサービスの全体像を確実に把握することが重要です。  Data Factory を使用すると、データ パイプラインの正常性をエンド ツー エンドで迅速に評価し、問題を特定して、必要に応じて是正措置を実行できます。また、どのソースのデータでも、データ系列やデータ間の関係を視覚的に追跡できます。ジョブ実行の課金履歴、システムの正常性、および依存関係を、1 つの監視ダッシュボードからすべて把握できます。
- **変換されたデータから豊富な洞察を獲得。**
組織が回答を必要とする疑問の絶え間ない変化に対応し、データの生成の準備が整うタイミングを常に把握できます。  使用するタイミングに合わせて信頼できる情報を生成し、ビジネスに関する洞察を向上させる能力を強化できます。データ パイプラインを使用して、変換したデータをクラウドから SQL Server などの内部設置型ソースに送信することも、クラウド ストレージ ソースに保持して、BI ツールや分析ツールなどのアプリケーションで使用することもできます。

開発者は現在、Data Factory のメリットを活用するために、個々のデータ パイプライン、ストレージ サービス、コンピューティング サービスと直接やり取りしています。今後は Data Factory サービスの進化に伴って、ストレージ サービスや処理サービスを追加し、コンピューティングおよびストレージのサービスとデータ パイプラインを "ハブ" としてグループ化する新しいメカニズムを導入する予定です。以下では、導入予定のハブについて説明します。この新しいコンセプトは、今後のリリースに先駆けて、サービス全体を通して登場します。

Azure Data Factory のハブは、ストレージ サービスとコンピューティング サービスのコンテナーであると同時に (どちらも "リンクされたサービス" と呼びます)、それらのリソースを使用したり実行の基盤としたりするデータ パイプラインのコンテナーでもあります。ハブ コンテナーを使用して、Data Factory を論理的なグループやドメイン別のグループに分割できます。たとえば、企業は、"米国西部 Azure ハブ" を作成して、米国西部データ センター向けのリンクされたサービスとパイプラインを管理したり、"営業部門 EDW ハブ" を作成して、営業部門 EDW のデータの入力と処理に関連するすべてのリンクされたサービスとパイプラインを管理したりすることができます。ハブの重要な特性は、パイプラインが単一のハブ上で実行されることです。つまり、パイプラインを定義する際、パイプライン内のテーブルまたはアクティビティによって参照されるすべてのリンクされたサービスには、パイプライン自体と同じハブ名を設定する必要があります。

ハブは、パイプラインが (使用している個々のサービスやテーブルではなく) 1 つのハブしか参照できないようにストレージとコンピューティングをカプセル化することができます。さらに、ハブはポリシーを使用してパイプラインを実行する場所を指定できます。これには、重要な効果がいくつかあります。1 つは、リンクされたサービスをハブに追加できるためスケールアップが容易になり、これらの新しいリンクされたサービス間でパイプラインを負荷分散できる点です。また、複数のハブでパイプラインの定義を再利用することもできます。

## シナリオ
データ パイプラインのシナリオは、内部設置型システム、クラウド システム、SaaS システムだけでなく、ストリーミングやバッチ ベースのデータ生成環境も対象とし、広範囲にわたっています。このセクションでは、Azure Data Factory が現在サポートしているシナリオの例をいくつか紹介します。これらは今後、ハブのシナリオとして引き続き発展していく予定です。

###シナリオ #1: データ ソースをデータ ハブに集約する
![Source the Data Hub][image-data-factory-introduction-secenario1-source-datahub]

企業が所有するデータは多種多様で、ソースもさまざまです。  情報生成システム構築の最初のステップは、SaaS サービス、ファイル共有、FTP、Web サービスなど、必要なすべてのデータ ソースと処理の機能に接続し、必要に応じて後続の処理機能にデータを移動させることです。

Data Factory を使用していない企業では、カスタムのデータ移動コンポーネントをビルドするか、これらのデータ取得と処理を統合するカスタム サービスを作成しなければなりません。このような作業にはコストがかかり、システムの統合や保守が容易ではありません。エンタープライズ クラスの監視やアラート、完全に管理されたサービスが提供できるような制御機能は、多くの場合望めません。
  
Azure Data Factory を使用すると、データの保存と処理のサービスはハブ コンテナーに集約され、コンピューティングと保存のアクティビティが円滑化、最適化されて、リソース消費の統合管理が可能になり、必要に応じてデータ移動のサービスを利用できます。

###シナリオ #2: 情報生成を運用する

運用のシナリオは、データ取得シナリオの次に進むべきステップです。データがハブに集まったら、データ パイプラインを作成し、運用する必要があります。これにより、保守しやすい管理されたスケジュールで、信頼性の高い方法でデータ変換を実行し、信頼できるデータを運用環境に提供できます。Azure Data Factory のデータ変換は、Hadoop (Azure HDInsight) 上で実行される Hive、Pig、およびカスタム C# 処理を通じて行われます。この変換は、データのクリーニングや機密データ フィールドのマスキングなど、複雑な方法を用いるさまざまなデータ操作に使用できます。  通常、情報生成の運用は、複雑で保守が難しいインフラストラクチャやカスタム サービスで実現されるため、実装、管理、拡張、トラブルシューティング、それらのソリューションのバージョン管理で多くの問題が発生しがちです。
  
Data Factory を完全に管理されたサービスとして使用すれば、ユーザーはパイプラインを運用する際に 1 回限りのスケジュールや複合的な定期スケジュールで定義するだけでよく、調整は Data Factory サービスが直接対応します。ハブを使用すれば、ハブ内のすべてのデータと処理を対象としたクラスター管理がユーザーに代わって実行されるため、ユーザーはインフラストラクチャ管理の変革に関する分析に集中できます。  不安定なカスタム サービスの操作に伴う問題は Azure Data Factory によって払拭されるため、企業は信頼できる情報を確実で再現可能な方法で生成できます。


###シナリオ #3: 情報生成とデータ探索を統合する
BI に関する従来のアプローチとテクノロジは、"信頼できる情報源" を提供する一方で、ほとんどの場合、変更要求プロセスが慎重に制御されているために要求のバックログが絶えず発生するという重大な副作用が付きものです。絶えず変化するビジネス上の疑問に対応するには、企業が情報利用システムから情報生成システムに接続できる優れた柔軟性が必要です。Azure Data Factory は、情報生成向けに効率化されたデータ パイプラインを使用してこれらのシステムへの接続に関する課題に対処し、使用しやすい形式で信頼できる最新のデータを提供して情報利用の課題を解決します。
  
Azure Data Factory は、生成されたデータをシンプルに利用できるように、次の機能をサポートしています。

- 既存の BI ツール (Excel や Tableau など) で利用できるように、生成されたデータ資産をリレーショナル データ マートに (1 回だけ、またはスケジュールを設定して) 簡単に移動。
- Data Factory によって生成されたデータ資産を Excel の Power Query から直接利用。

Power Query からのデータの利用については、次のトピックを参照してください。 

- [Power Query: Microsoft Azure テーブル ストレージに接続する] [Power-Query-Azure-Table]
- [Power Query: Windows Azure Blob ストレージに接続する] [Power-Query-Azure-Blob]
- [Power Query: Microsoft Azure SQL データベースに接続する] [Power-Query-Azure-SQL]
- [Power Query: SQL Server データベースに接続する][Power-Query-OnPrem-SQL] 

## アプリケーション モデル
次の図は、Azure Data Factory でサポートされているアプリケーション モデルを示しています。

![Application Model][image-data-factory-application-model]

Azure Data Factory には 3 つの情報生成段階があります。


- **接続と収集**。この段階では、さまざまなデータ ソースからデータがデータ ハブにインポートされます。
- **変換と強化**。この段階では、収集されたデータが処理されます。
- **発行**。この段階では、BI ツールや分析ツールなどのアプリケーションで使用できるように、データが発行されます。

開発者は Data Factory を使用して**パイプライン**を作成できます。パイプラインとは、データの移動や処理の**アクティビティ**の集合体です。アクティビティは、1 つ以上の入力**データセット**を受け付け、1 つ以上の出力データセットを生成する処理を指します。パイプラインは 1 回だけ実行することも、柔軟なスケジュール (1 時間ごと、毎日、毎週など) で実行することもできます。**データセット**とは、データの名前付きビューです。記述されるデータは、単純なバイト、CSV ファイルのような半構造化データから、**テーブル**や**モデル**までさまざまです。

データ移動**アクティビティ**で構成された**パイプライン** (例: コピー アクティビティ) は、多くの場合、組織が使用しているすべての**データ ソース** (データベース、ファイル、SaaS サービスなど) からデータを**データ ハブ**にインポート/エクスポートする際に使用されます。
    
データが**ハブ**に集まると、ハブのコンピューティング サービスにホストされた**パイプライン**を使用して、(BI ツール、アプリケーション、顧客などによる) 利用に適した形式にデータが変換されます。  
  
最終的に、**パイプライン**は、(図に示すように) 1 つのパイプラインの出力**データセット**が別のパイプラインの入力になるように連結できます。これにより、複雑なデータ フローを**パイプライン**に組み込んで、1 つのデータ ハブ内で実行したり、複数のハブに展開したりできます。**パイプライン**をこのように使用することで、組織は、管理が容易な単一の Data Factory という視点で、最高水準の内部設置型、クラウド、およびサービスとしてのソフトウェア (SaaS) サービスを構成するために必要な要素が揃います。 


##用語集
このセクションでは、Azure Data Factory に関連する用語について説明します。

### Data Factory

**Azure Data Factory** は完全に管理されたサービスで、データの保存、処理、移動の各サービスを効率的かつスケーラブルで信頼性の高いデータ生成パイプラインとして構成します。 

Data Factory サービスは、**データセット**を使用、生成、管理、発行します。1 つの Azure サブスクリプションで 1 つ以上の Azure Data Factory インスタンスを利用できます。1 つの Azure Data Factory を 1 つ以上のストレージ サービスまたはコンピューティング サービスにリンクできます (リンクされたサービスと呼びます)。
 
Azure Data Factory に 1 つ以上のパイプラインを作成し、Azure HDInsight などのリンクされたコンピューティング サービスを使用して、リンクされたストレージ内のデータを処理できます。Azure Data Factory 自体にデータは格納されません。データは、Data Factory の外部にある、ユーザーが既に使用しているストレージ システムに保存されます。


### リンクされたサービス
リンクされたサービスとは、Azure Data Factory にリンクされているサービスです。リンクされたサービスは、次のいずれかです。


- Azure ストレージ、Azure SQL データベース、内部設置型 SQL Server データベースなどのデータ ストア
- Azure HDInsight などのコンピューティング サービス

データ ストアとは、データまたはデータセットのコンテナーです。Azure HDInsight は、現時点でサポートされている唯一のコンピューティング サービスです。まず、データ ストアをポイントするリンクされたサービスを作成し、次に、そのデータ ストアのデータを表すデータ セットを定義します。 

### データ セット
データの名前付きビューです。記述されるデータは、単純なバイト、CSV ファイルのような半構造化データから、リレーショナル テーブルやモデルまでさまざまです。**テーブル**とは、スキーマを持つ四角形のデータ セットです。

###パイプライン
Azure Data Factory の**パイプライン**は、リンクされたコンピューティング サービスを使用して、リンクされたストレージ サービス内のデータを処理します。パイプラインは、一連のアクティビティで構成されます。各アクティビティは、特定の処理操作を実行します。たとえば、コピー アクティビティは、ソース ストレージからターゲット ストレージにデータをコピーします。Hive/Pig アクティビティは、Azure HDInsight クラスターを使用して、Hive クエリや Pig スクリプトによるデータ処理を実行します。

Azure Data Factory インスタンスを作成するための一般的な手順は、次のとおりです。

1. Data Factory を作成する。
2. データ ストアまたはコンピューティング サービスごとにリンクされたサービスを作成する。
3. 入力データセットと出力データセットを作成する
4. パイプラインを作成する。 

###アクティビティ
パイプラインにおけるデータ処理の手順です。1 つ以上の入力データセットを受け取り、1 つ以上の出力データセットを生成します。アクティビティは実行環境で実行され (例: Azure HDInsight クラスター)、Azure Data Factory インスタンスに関連付けられたデータ ストアに対してデータの読み取り/書き込みを行います。
 
###データ ハブ
Azure データ ハブは、データのストレージおよびコンピューティング サービスのコンテナーです。たとえば、ストレージでは HDFS を使用する Hadoop クラスターがデータ ハブで、コンピューティング (処理) では Hive/Pig がデータ ハブです。同様に、エンタープライズ データ ウェアハウス (EDW) をデータ ハブとしてモデル化できます (ストレージとしてはデータベース、コンピューティング サービスとしてはストアド プロシージャや ETL ツール)。パイプラインは、データ ハブ内でデータ ストアを使用し、コンピューティング リソースを実行します。現時点でサポートされているのは、HDInsight ハブのみです。

データ ハブを使用して、Data Factory を論理的なグループやドメイン別のグループに分割できます。たとえば、"米国西部 Azure ハブ" を作成して、米国西部データ センター向けのリンクされたサービス (データ ストアとコンピューティング) とパイプラインを管理したり、"営業部門 EDW ハブ" を作成して、営業部門エンタープライズ データ ウェアハウスのデータの入力と処理に関連するすべてのリンクされたサービスとパイプラインを管理したりできます。

ハブの重要な特性は、パイプラインが 1 つのハブで実行されることです。つまり、パイプラインを定義する際、パイプライン内のテーブルまたはアクティビティによって参照されるすべてのリンクされたサービスには、パイプライン自体と同じハブ名を設定する必要があります。リンクされたサービスに HubName プロパティが指定されていない場合、リンクされたサービスは "Default" ハブに配置されます。

###スライス
Azure Data Factory 内のテーブルは、時間軸のスライスで構成されます。スライスの幅は、スケジュール (毎時/毎日) によって決まります。たとえば、スケジュールが "毎時" のときは、パイプラインの開始時から終了時までに、スライスが 1 時間ごとに作成されます。  

IT プロフェッショナルは、スライスを使用して、特定の期間に対する全データのサブセットを利用できます (例: 午後 1 時から午後 2 時の間に生成されたスライス)。指定した期間内のすべてのダウンストリーム データ スライスを表示し、エラー発生時のスライスを再実行できます。

この実行とは、スライスの処理単位です。再試行する場合や、エラー発生時にスライスを再実行する場合に、1 つのスライスを 1 回以上実行できます。スライスは、開始時刻によって識別されます。 

###Data Management Gateway
Microsoft Data Management Gateway は、内部設置型のデータ ソースをクラウド サービスで使用できるように接続するソフトウェアです。内部設置型のデータ ソースをリンクされたサービスとして追加するには、企業環境に少なくとも 1 つのゲートウェイをインストールし、Azure Data Factory ポータルに登録する必要があります。


##次のステップ
[Data Factory を使ってみましょう][datafactory-getstarted]。リンク先の記事では、Azure BLOB から Azure SQL データベースにデータをコピーする Azure Data Factory サンプルの作成方法についての詳細なチュートリアルが記載されています。

[Power-Query-Azure-Table]: http://office.microsoft.com/en-001/excel-help/connect-to-microsoft-azuretable-storage-HA104122607.aspx
[Power-Query-Azure-Blob]: http://office.microsoft.com/en-001/excel-help/connect-to-microsoft-azure-blob-storage-HA104113447.aspx
[Power-Query-Azure-SQL]: http://office.microsoft.com/en-001/excel-help/connect-to-a-microsoft-azure-sql-database-HA104019809.aspx
[Power-Query-OnPrem-SQL]: http://office.microsoft.com/en-001/excel-help/connect-to-a-sql-server-database-HA104019808.aspx


[datafactory-getstarted]: ../data-factory-get-started/

[image-data-factory-introduction-traditional-ETL]: ./media/data-factory-introduction/TraditionalETL.PNG

[image-data-factory-introduction-todays-diverse-processing-landspace]:./media/data-factory-introduction/TodaysDiverseDataProcessingLandscape.PNG

[image-data-factory-introduction-secenario1-source-datahub]:./media/data-factory-introduction/Scenario1SourceDataHub.png

[image-data-factory-introduction-secenario2-operationalize-infoproduction]:./media/data-factory-introduction/Scenario2-OperationalizeInformationProduction.png

[image-data-factory-application-model]:./media/data-factory-introduction/DataFactoryApplicationModel.png

[image-data-factory-data-flow]:./media/data-factory-introduction/DataFactoryDataFlow.png



